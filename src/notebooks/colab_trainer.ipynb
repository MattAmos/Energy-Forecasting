{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6BPpaGLQc5RJvTnlB8CTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaulgeous/Energy-Forecasting/blob/main/colab_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Conv1D, Flatten, GRU, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import absl.logging\n",
        "from sklearn.metrics import mean_squared_error as mse, r2_score, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Categorical\n",
        "from skopt import dump, load\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import math\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ],
      "metadata": {
        "id": "-4EMjoYoTiLm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline_model():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(1, 'linear'))\n",
        "\n",
        "  model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), \n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def build_simple_model():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "\n",
        "  model.add(Dense(1, 'linear'))\n",
        "\n",
        "  model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), \n",
        "              metrics=['mean_squared_error'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_simple_model(model, X_frame, y_frame, split, data_epochs, batch_size, y_scaler):\n",
        "\n",
        "  length = X_frame.shape[0]\n",
        "  X_train = X_frame[:int(length*split),:]\n",
        "  y_train = y_frame[:int(length*split)]\n",
        "\n",
        "  X_test = X_frame[int(length*split):,:]\n",
        "  y_test = y_frame[int(length*split):]\n",
        "\n",
        "  model.fit(X_train, y_train, verbose=0, epochs=data_epochs,\n",
        "                  batch_size=batch_size, validation_split=0.2)\n",
        "  preds = model.predict(X_test, verbose=0)\n",
        "  preds = y_scaler.inverse_transform(preds)\n",
        "  y_test = y_scaler.inverse_transform(y_test)\n",
        "\n",
        "  return mse(y_test, preds, squared=True)\n",
        "\n",
        "\n",
        "def simple_evaluate(future, set_name, X_train, y_train, epochs, batch_size):\n",
        "\n",
        "  time_start = time.time()\n",
        "\n",
        "  model_directory = \"\"\n",
        "\n",
        "  absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "  tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "  model = build_baseline_model()\n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "\n",
        "  model.save(model_directory + \"\" + set_name + \"_baseline_\" + str(future))\n",
        "  \n",
        "  print(\"Finished evaluating baseline for future {0}\".format(future))\n",
        "\n",
        "  time_end = time.time()\n",
        "\n",
        "  return time_end - time_start\n",
        "\n",
        "\n",
        "def simple_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler):\n",
        "\n",
        "  folder_path = os.getcwd()\n",
        "  model_directory = \"\" \n",
        "  csv_directory = \"\"\n",
        "\n",
        "  model = load_model(model_directory + \"\" + set_name + \"_baseline_\" + str(future))\n",
        "  predictions = model.predict(X_test)\n",
        "  predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "  y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "  make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"Baseline\")\n",
        "\n",
        "  print(\"Finished running baseline prediction on future window {0}\".format(future))\n",
        "\n",
        "  metric_outputs = get_metrics(predictions, y_test, 0, \"Baseline\")\n",
        "  return metric_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "zzuGkjtrTVKs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(predictions, actual, cv, model_name):\n",
        "\n",
        "    MSE = mse(actual, predictions, squared=True)\n",
        "    MAE = mae(actual, predictions)\n",
        "    MAPE = mape(actual, predictions)\n",
        "    RMSE = mse(actual, predictions, squared=False)\n",
        "    R2 = r2_score(actual, predictions)\n",
        "    if cv:\n",
        "        metrics = {model_name + '_RMSE': RMSE, model_name + '_R2': R2, model_name +'_MSE': MSE, \n",
        "                   model_name + '_MAE': MAE, model_name + '_MAPE': MAPE}\n",
        "    else:\n",
        "        metrics = {'RMSE': RMSE, 'R2': R2, 'MSE': MSE, 'MAE': MAE, 'MAPE': MAPE}\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def cross_val_metrics(total_metrics, set_name, future, model_name):\n",
        "\n",
        "    csv_directory = \"\"\n",
        "    df = pd.DataFrame(total_metrics)\n",
        "    df.to_csv(csv_directory + \"\" + set_name + \"_\" + model_name + \"_cv_metrics_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        "\n",
        "def normalise_metrics(metrics, training):\n",
        "\n",
        "    rmse = [key[\"RMSE\"] for key in metrics]\n",
        "    mse = [key[\"MSE\"] for key in metrics]\n",
        "    mae = [key[\"MAE\"] for key in metrics]\n",
        "    r2 = [key[\"R2\"] for key in metrics]\n",
        "\n",
        "    metrics_sets = {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"R2\": r2}\n",
        "\n",
        "    if training:\n",
        "        time = [key[\"TIME\"] for key in metrics]\n",
        "        metrics_sets = {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae, \"R2\": r2, \"TIME\": time}\n",
        "\n",
        "    for name, set in metrics_sets.items():\n",
        "        top = max(set)\n",
        "        counter = 0\n",
        "\n",
        "        while top > 10:\n",
        "            top /= 10\n",
        "            set = [entry / 10 for entry in set]\n",
        "            counter += 1\n",
        "        \n",
        "        i = 0\n",
        "\n",
        "        for key in metrics:\n",
        "            key[name] = set[i]\n",
        "            i += 1\n",
        "\n",
        "    return metrics\n",
        "\n",
        "    \n",
        "# This is going to have to be rejigged\n",
        "def make_metrics_csvs(csv_directory, metrics, set_name, future, training):\n",
        "\n",
        "    for model_name, metric_outputs in metrics.items():\n",
        " \n",
        "        if not os.path.exists(csv_directory + \"\" + set_name + \"_metrics_\" + str(future) + \".csv\"):\n",
        "            metrics = pd.DataFrame({\"Model\": [], \"Metric\": [], \"Value\": []})\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"RMSE\", \"Value\": metric_outputs.get(\"RMSE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAE\", \"Value\": metric_outputs.get(\"MAE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAPE\", \"Value\": metric_outputs.get(\"MAPE\")}\n",
        "            metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"R2\", \"Value\": metric_outputs.get(\"R2\")}\n",
        "            if training:\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"TIME\", \"Value\": metric_outputs.get(\"TIME\")}\n",
        "\n",
        "            metrics.to_csv(csv_directory + \"\" + set_name + \"_metrics_\" + str(future) + \".csv\", index=False)\n",
        "        else:\n",
        "\n",
        "            metrics = pd.read_csv(csv_directory + \"\" + set_name + \"_metrics_\" + str(future) + \".csv\")\n",
        "\n",
        "            if model_name in metrics['Model'].values:\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"RMSE\"), 'Value'] = metric_outputs.get(\"RMSE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"MAE\"), 'Value'] = metric_outputs.get(\"MAE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"MAPE\"), 'Value'] = metric_outputs.get(\"MAPE\")\n",
        "                metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"R2\"), 'Value'] = metric_outputs.get(\"R2\")\n",
        "                if training:\n",
        "                    metrics.loc[(metrics['Model'] == model_name) & (metrics[\"Metric\"] == \"TIME\"), 'Value'] = metric_outputs.get(\"TIME\")\n",
        "            else:\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"RMSE\", \"Value\": metric_outputs.get(\"RMSE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAE\", \"Value\": metric_outputs.get(\"MAE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"MAPE\", \"Value\": metric_outputs.get(\"MAPE\")}\n",
        "                metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"R2\", \"Value\": metric_outputs.get(\"R2\")}\n",
        "                if training:\n",
        "                    metrics.loc[len(metrics)] = {\"Model\": model_name, \"Metric\": \"TIME\", \"Value\": metric_outputs.get(\"TIME\")}\n",
        "            metrics.to_csv(csv_directory + \"\" + set_name + \"_metrics_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        "\n",
        "def make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, model_name):\n",
        "\n",
        "    if not os.path.exists(csv_directory + \"\" + set_name + \"_performances_\" + str(future) + \".csv\"):\n",
        "        performances = pd.DataFrame({\"Date\":pred_dates_test, \"Actual\": y_test, model_name: predictions})\n",
        "        performances = performances.iloc[-1000:,:]\n",
        "        performances.to_csv(csv_directory + \"\" + set_name + \"_performances_\" + str(future) + \".csv\", index=False)\n",
        "    else:\n",
        "        performances = pd.read_csv(csv_directory + \"\" + set_name + \"_performances_\" + str(future) + \".csv\")\n",
        "        performances[model_name] = predictions[-1000:]\n",
        "        performances.to_csv(csv_directory + \"\" + set_name + \"_performances_\" + str(future) + \".csv\", index=False)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "l9PRWGA4TScj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collapse_columns(data):\n",
        "    data = data.copy()\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = data.columns.to_series().apply(lambda x: \"__\".join(x))\n",
        "    return data\n",
        "    \n",
        "def create_dataset_2d(input, win_size):\n",
        "    \n",
        "    np_data = np.array(input.copy())\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for i in range(len(np_data)-win_size):\n",
        "        row = [r for r in np_data[i:i+win_size]]\n",
        "        X.append(row)\n",
        "\n",
        "    X = np.array(X)\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    return X\n",
        "    \n",
        "\n",
        "def create_dataset_3d(input, win_size):\n",
        "    \n",
        "    np_data = np.array(input.copy())\n",
        "\n",
        "    X = []\n",
        "\n",
        "    for i in range(len(np_data)-win_size):\n",
        "        row = [r for r in np_data[i:i+win_size]]\n",
        "        X.append(row)\n",
        "\n",
        "    return np.array(X)\n",
        "\n",
        "\n",
        "def load_datasets(csv_directory, set_name, future):\n",
        "\n",
        "    data_name = csv_directory + \"\" + set_name + \"_data_\" + str(future) + \".csv\"\n",
        "    output_name = csv_directory + \"\" + set_name + \"_outputs_\" + str(future) + \".csv\"\n",
        "\n",
        "    data = pd.read_csv(data_name).set_index(\"Date\")\n",
        "    outputs = pd.read_csv(output_name).set_index(\"Date\")\n",
        "\n",
        "    return data, outputs\n",
        "\n",
        "\n",
        "def finalise_data(data, outputs, target, best_results):\n",
        "\n",
        "    pred_dates = outputs.index\n",
        "\n",
        "    pca_dim = best_results.get(\"pca_dimensions\")\n",
        "    y_scaler = None\n",
        "    \n",
        "    if best_results.get(\"scaler\") == \"minmax\":\n",
        "        X_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        y_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        data = X_scaler.fit_transform(data)\n",
        "        outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "    elif best_results.get(\"scaler\") == \"standard\":\n",
        "        X_scaler = StandardScaler()\n",
        "        y_scaler = StandardScaler()\n",
        "        data = X_scaler.fit_transform(data)\n",
        "        outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "    if pca_dim == \"None\":\n",
        "        pca = PCA()\n",
        "        data = pca.fit_transform(data)\n",
        "    elif pca_dim == \"mle\":\n",
        "        pca = PCA(n_components=\"mle\")\n",
        "        data = pca.fit_transform(data)\n",
        "    elif pca_dim != \"NO_PCA\":\n",
        "        pca = PCA(n_components=pca_dim)\n",
        "        data = pca.fit_transform(data)\n",
        "\n",
        "    X_frame = np.array(data)\n",
        "    y_data = np.array(outputs)\n",
        "\n",
        "    return X_frame, y_data, pred_dates, y_scaler\n",
        "\n",
        "\n",
        "def data_cleaning_pipeline(data_in, outputs_in, cleaning_parameters, target, split, data_epochs, batch_size, csv_directory):\n",
        "\n",
        "    best_results = {\"MSE\": [math.inf], \"scaler\": [None], \"pca_dimensions\": [None]}\n",
        "\n",
        "    for scale_type in cleaning_parameters.get('scalers'):\n",
        "            for pca_dim in cleaning_parameters.get('pca_dimensions'):\n",
        "\n",
        "                data = data_in.copy()\n",
        "                outputs = outputs_in.copy()\n",
        "\n",
        "                if scale_type == 'minmax':\n",
        "                    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "                    y_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "                    data = X_scaler.fit_transform(data)\n",
        "                    outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "                elif scale_type == 'standard':\n",
        "                    X_scaler = StandardScaler()\n",
        "                    y_scaler = StandardScaler()\n",
        "                    data = X_scaler.fit_transform(data)\n",
        "                    outputs = y_scaler.fit_transform(outputs[[target]])\n",
        "\n",
        "                if pca_dim == None:\n",
        "                    pca = PCA()\n",
        "                    data = pca.fit_transform(data)\n",
        "                elif pca_dim == -math.inf:\n",
        "                    pca = PCA(n_components=\"mle\")\n",
        "                    data = pca.fit_transform(data)\n",
        "                elif pca_dim != math.inf:\n",
        "                    pca = PCA(n_components=pca_dim)\n",
        "                    data = pca.fit_transform(data)\n",
        "\n",
        "                X_frame = np.array(data)\n",
        "                y_frame = np.array(outputs)\n",
        "                \n",
        "                model = build_simple_model()\n",
        "                mse = train_simple_model(model, X_frame, y_frame, split, data_epochs, batch_size, y_scaler)\n",
        "                print(\"Trained scale:{0} dim:{1}\".format(scale_type, pca_dim))\n",
        "                if mse < best_results.get(\"MSE\"):\n",
        "                    if pca_dim == None:\n",
        "                        pca_dim = \"None\"\n",
        "                    elif pca_dim == math.inf:\n",
        "                        pca_dim = \"NO_PCA\"\n",
        "                    elif pca_dim == -math.inf:\n",
        "                        pca_dim = \"mle\"\n",
        "                    best_results[\"MSE\"][0] = mse\n",
        "                    best_results[\"pca_dimensions\"][0] = pca_dim\n",
        "                    best_results[\"scaler\"][0] = scale_type\n",
        "\n",
        "    results_data = pd.DataFrame.from_dict(best_results)\n",
        "    results_data.to_csv(csv_directory + \"best_data_parameters.csv\", index=False)\n",
        "\n",
        "    best_results = {\"MSE\": best_results.get(\"MSE\"), \"scaler\": best_results.get(\"scaler\")[0], \"pca_dimensions\": best_results.get(\"pca_dimensions\")[0]}\n",
        "    return best_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def feature_adder(csv_directory, file_path, target, trend_type, future, epd,  set_name):\n",
        "\n",
        "    data = pd.read_excel(file_path).set_index(\"Date\")\n",
        "    data = collapse_columns(data)\n",
        "\n",
        "    data['PrevDaySameHour'] = data[target].copy().shift(epd)\n",
        "    data['PrevWeekSameHour'] = data[target].copy().shift(epd*7)\n",
        "    data['Prev24HourAveLoad'] = data[target].copy().rolling(window=epd*7, min_periods=1).mean()\n",
        "    data['Weekday'] = data.index.dayofweek\n",
        "\n",
        "    if 'Holiday' in data.columns.values:\n",
        "        data.loc[(data['Weekday'] < 5) & (data['Holiday'] == 0), 'IsWorkingDay'] = 1\n",
        "        data.loc[(data['Weekday'] > 4) | (data['Holiday'] == 1), 'IsWorkingDay'] = 0\n",
        "    else:\n",
        "        data.loc[data['Weekday'] < 5, 'IsWorkingDay'] = 1\n",
        "        data.loc[data['Weekday'] > 4, 'IsWorkingDay'] = 0\n",
        "\n",
        "    dec_daily = seasonal_decompose(data[target], model=trend_type, period=epd)\n",
        "    data['IntraDayTrend'] = dec_daily.trend\n",
        "    data['IntraDaySeasonal'] = dec_daily.seasonal\n",
        "    data['IntraDayTrend'] = data['IntraDayTrend'].shift(epd)\n",
        "    data['IntraDaySeasonal'] = data['IntraDaySeasonal'].shift(epd)\n",
        "\n",
        "    dec_weekly = seasonal_decompose(data[target], model=trend_type, period=epd*7)\n",
        "    data['IntraWeekTrend'] = dec_weekly.trend\n",
        "    data['IntraWeekSeasonal'] = dec_weekly.seasonal\n",
        "    data['IntraWeekTrend'] = data['IntraWeekTrend'].shift(epd*7)\n",
        "    data['IntraWeekSeasonal'] = data['IntraWeekSeasonal'].shift(epd*7)\n",
        "\n",
        "    data[target] = y = data[target].shift(-epd*future)\n",
        "    data = data.dropna(how='any', axis='rows')\n",
        "    y = data[target].reset_index(drop=True)\n",
        "    # y = data[target].shift(-epd*future).reset_index(drop=True)\n",
        "    # y = y.dropna(how='any', axis='rows')\n",
        "\n",
        "    future_dates = pd.Series(data.index[future*epd:])\n",
        "    outputs = pd.DataFrame({\"Date\": future_dates, \"{0}\".format(target): y})\n",
        "\n",
        "    # future > 10 needs addressing - it is not yet implemented\n",
        "    if future > 10:\n",
        "        data = data[['DryBulb', 'DewPnt', 'Prev5DayHighAve', 'Prev5DayLowAve', 'Hour', 'Weekday', 'IsWorkingDay']]\n",
        "    else:\n",
        "        data = data.drop(\"{0}\".format(target), axis=1)\n",
        "\n",
        "    data_name = csv_directory + \"\" + set_name + \"_data_\" + str(future) + \".csv\"\n",
        "    output_name = csv_directory + \"\" + set_name + \"_outputs_\" + str(future) + \".csv\"\n",
        "\n",
        "    data.to_csv(data_name)\n",
        "    outputs.to_csv(output_name, index=False)\n",
        "\n",
        "    print(\"Saved future window {0} to csvs\".format(future))\n",
        "\n",
        "    return data, outputs\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngy-HHeQTXhW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8H1TyNVDTEUU"
      },
      "outputs": [],
      "source": [
        "def bnn_kt_model(hp):\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_reg = hp.Float(\"reg\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
        "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    \n",
        "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=200, step=10)\n",
        "\n",
        "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
        "    layers = 0\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    while neuron_count > 5 and layers < 5:\n",
        "\n",
        "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
        "        model.add(Dropout(hp_dropout))\n",
        "        layers += 1\n",
        "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate), \n",
        "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def bnn_save_plots(history, graphs_directory, set_name, future):\n",
        "\n",
        "    graph_names = {\"Loss\": \"loss\", \"MAE\": \"mean_absolute_error\", \n",
        "                   \"MSE\": \"mean_squared_error\", \"MAPE\": \"mean_absolute_percentage_error\"}\n",
        "    \n",
        "    for name, value in graph_names.items():\n",
        "        graph_loc = graphs_directory + \"\" + set_name + \"_basic_nn_\" + str(future) + \"_\" + name + \".png\"\n",
        "        if os.path.exists(graph_loc):\n",
        "            os.remove(graph_loc)\n",
        "\n",
        "        val_name = \"val_\" + value\n",
        "        plt.plot(history.history[value])\n",
        "        plt.plot(history.history[val_name])\n",
        "        plt.title('Basic NN {0} for {1} {2}'.format(name, set_name, future))\n",
        "        plt.ylabel(name)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.savefig(graphs_directory + \"\" + set_name + \"_basic_nn_\" + str(future) + \"_\" + name + \".png\")\n",
        "    \n",
        "\n",
        "def bnn_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd):\n",
        "\n",
        "    tuner = kt.Hyperband(bnn_kt_model, objective='mean_absolute_percentage_error', max_epochs=epochs, factor=3, \n",
        "                        directory=model_directory + \"\" + set_name + \"_kt_dir\", project_name='kt_model_' + str(future), \n",
        "                        overwrite=True)\n",
        "\n",
        "    monitor = EarlyStopping(monitor='mean_absolute_percentage_error', min_delta=1, patience=5, verbose=0, mode='auto', \n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n",
        "                callbacks=[monitor])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Split on a 3 monthly basis\n",
        "    tss = TimeSeriesSplit(n_splits=10, test_size=epd*90, gap=0)\n",
        "    fold = 0\n",
        "    total_metrics = {}\n",
        "\n",
        "    for train_idx, val_idx in tss.split(X_train, y_train):\n",
        "\n",
        "        fold_name = \"Fold_\" + str(fold)\n",
        "        X_t = X_train[train_idx]\n",
        "        X_v = X_train[val_idx]\n",
        "        y_t = y_train[train_idx]\n",
        "        y_v = y_train[val_idx]\n",
        "        \n",
        "        if fold == 9:\n",
        "            history = model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "                    batch_size=batch_size, validation_data=(X_v, y_v))\n",
        "            graphs_directory = \"\"\n",
        "            bnn_save_plots(history, graphs_directory, set_name, future)\n",
        "            model.save(model_directory + \"\" + set_name + \"_basic_nn_\" + str(future))\n",
        "        \n",
        "        model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "                    batch_size=batch_size)\n",
        "        preds = model.predict(X_v, verbose=0)\n",
        "        preds = y_scaler.inverse_transform(preds)\n",
        "        metrics = get_metrics(preds, y_v, 1, \"Basic_nn\")\n",
        "        total_metrics[fold_name] = metrics\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "    cross_val_metrics(total_metrics, set_name, future, \"Basic_nn\")\n",
        "\n",
        "\n",
        "def bnn_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler):\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = folder_path + r\"\\models\"\n",
        "    csv_directory = folder_path + r\"\\csvs\"\n",
        "\n",
        "    model = load_model(model_directory + \"\" + set_name + \"_basic_nn_\" + str(future))\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"Basic_nn\")\n",
        "\n",
        "    print(\"Finished running basic prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"Basic_nn\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def bnn_evaluate(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = folder_path + r\"\\models\"\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    bnn_train_model(future, batch_size, epochs,\n",
        "            model_directory, set_name, X_train, y_train, y_scaler, epd)\n",
        "    \n",
        "    print(\"Finished evaluating basic nn for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    return time_end - time_start\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_kt_model(hp):\n",
        "\n",
        "    X = np.load(\"X_train_3d.npy\")\n",
        "\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_reg = hp.Float(\"reg\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    hp_dropout = hp.Float(\"dropout\", min_value=1e-3, max_value=0.5, sampling=\"linear\")\n",
        "    hp_neuron_pct = hp.Float('NeuronPct', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    hp_neuron_shrink = hp.Float('NeuronShrink', min_value=1e-3, max_value=1.0, sampling='linear')\n",
        "    \n",
        "    hp_l_layer_1 = hp.Int('l_layer_1', min_value=1, max_value=100, step=10)\n",
        "    hp_max_neurons = hp.Int('neurons', min_value=10, max_value=5000, step=10)\n",
        "\n",
        "    neuron_count = int(hp_neuron_pct * hp_max_neurons)\n",
        "    layers = 0\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer((X.shape[1], X.shape[2])))\n",
        "    model.add(LSTM(hp_l_layer_1, return_sequences=True, activity_regularizer=regularizers.l1(hp_reg)))\n",
        "    model.add(Dropout(hp_dropout))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    while neuron_count > 20 and layers < 20:\n",
        "\n",
        "        model.add(Dense(units=neuron_count, activation=hp_activation))\n",
        "        model.add(Dropout(hp_dropout))\n",
        "        layers += 1\n",
        "        neuron_count = int(neuron_count * hp_neuron_shrink)\n",
        "\n",
        "    model.add(Dense(1, 'linear'))\n",
        "\n",
        "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=hp_learning_rate), \n",
        "                metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def cnn_save_plots(history, graphs_directory, set_name, future):\n",
        "\n",
        "    graph_names = {\"Loss\": \"loss\", \"MAE\": \"mean_absolute_error\", \n",
        "                   \"MSE\": \"mean_squared_error\", \"MAPE\": \"mean_absolute_percentage_error\"}\n",
        "    \n",
        "    for name, value in graph_names.items():\n",
        "        graph_loc = graphs_directory + \"\" + set_name + \"_Complex_nn_\" + str(future) + \"_\" + name + \".png\"\n",
        "        if os.path.exists(graph_loc):\n",
        "            os.remove(graph_loc)\n",
        "\n",
        "        val_name = \"val_\" + value\n",
        "        plt.plot(history.history[value])\n",
        "        plt.plot(history.history[val_name])\n",
        "        plt.title('Complex NN {0} for {1} {2}'.format(name, set_name, future))\n",
        "        plt.ylabel(name)\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.savefig(graphs_directory + \"\" + set_name + \"_Complex_nn_\" + str(future) + \"_\" + name + \".png\")\n",
        "    \n",
        "\n",
        "def cnn_train_model(future, batch_size, epochs,\n",
        "                model_directory, set_name, X_train, y_train, y_scaler, epd):\n",
        "\n",
        "    tuner = kt.Hyperband(cnn_kt_model, objective='mean_absolute_percentage_error', max_epochs=epochs, factor=3, \n",
        "                        directory=model_directory + \"\" + set_name + \"_kt_dir\", project_name='kt_model_' + str(future), \n",
        "                        overwrite=True)\n",
        "\n",
        "    monitor = EarlyStopping(monitor='mean_absolute_percentage_error', min_delta=1, patience=5, verbose=0, mode='auto', \n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n",
        "                callbacks=[monitor])\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Split on a 3 monthly basis\n",
        "    tss = TimeSeriesSplit(n_splits=10, test_size=epd*90, gap=0)\n",
        "    fold = 0\n",
        "    total_metrics = {}\n",
        "\n",
        "    for train_idx, val_idx in tss.split(X_train, y_train):\n",
        "\n",
        "        fold_name = \"Fold_\" + str(fold)\n",
        "        X_t = X_train[train_idx]\n",
        "        X_v = X_train[val_idx]\n",
        "        y_t = y_train[train_idx]\n",
        "        y_v = y_train[val_idx]\n",
        "        \n",
        "        if fold == 9:\n",
        "            history = model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "                    batch_size=batch_size, validation_data=(X_v, y_v))\n",
        "            graphs_directory = \"\"\n",
        "            cnn_save_plots(history, graphs_directory, set_name, future)\n",
        "            model.save(model_directory + \"\" + set_name + \"_Complex_nn_\" + str(future))\n",
        "        \n",
        "        model.fit(X_t, y_t, verbose=0, epochs=epochs, callbacks=[monitor],\n",
        "                    batch_size=batch_size)\n",
        "        preds = model.predict(X_v, verbose=0)\n",
        "        preds = y_scaler.inverse_transform(preds)\n",
        "        metrics = get_metrics(preds, y_v, 1, \"Complex_nn\")\n",
        "        total_metrics[fold_name] = metrics\n",
        "\n",
        "        fold += 1\n",
        "\n",
        "    cross_val_metrics(total_metrics, set_name, future, \"Complex_nn\")\n",
        "\n",
        "\n",
        "\n",
        "def cnn_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler):\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = folder_path + r\"\\models\"\n",
        "    csv_directory = folder_path + r\"\\csvs\"\n",
        "\n",
        "    model = load_model(model_directory + \"\" + set_name + \"_Complex_nn_\" + str(future))\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = y_scaler.inverse_transform(predictions).reshape(-1)\n",
        "    y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"Complex_nn\")\n",
        "\n",
        "    print(\"Finished running complex prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"Complex_nn\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def cnn_evaluate(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = folder_path + r\"\\models\"\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "    tf.compat.v1.logging.set_verbosity(30)\n",
        "\n",
        "    cnn_train_model(future, batch_size, epochs,\n",
        "            model_directory, set_name, X_train, y_train, y_scaler, epd)\n",
        "    \n",
        "    print(\"Finished evaluating complex nn for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    return time_end - time_start"
      ],
      "metadata": {
        "id": "oQu_dy8BTKGw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rf_train_model(future, epochs, model_directory, set_name, X_train, y_train, epd):\n",
        "\n",
        "    tss = TimeSeriesSplit(n_splits=5, test_size=epd*90, gap=0)\n",
        "    estimator = RandomForestRegressor()\n",
        "\n",
        "    search_space = {\n",
        "        \"max_depth\": (10, 1200),\n",
        "        \"min_samples_leaf\": (0.001, 0.5, \"uniform\"),\n",
        "        \"min_samples_split\": (0.001, 1.0, \"uniform\"),\n",
        "        \"n_estimators\": (5, 5000),\n",
        "        \"criterion\": Categorical([\"squared_error\"]),\n",
        "        \"max_features\": Categorical(['sqrt', 'log2', None]),\n",
        "    }\n",
        "\n",
        "    model = BayesSearchCV(\n",
        "        estimator=estimator,\n",
        "        search_spaces=search_space,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        cv=tss,\n",
        "        n_jobs=-1,\n",
        "        n_iter=epochs,\n",
        "        verbose=0,\n",
        "        refit=True,\n",
        "    )\n",
        "\n",
        "    model = model.fit(X_train, y_train)\n",
        "    dump(model, model_directory + \"\" + set_name + \"_rf_\" + str(future) + \".pkl\")\n",
        "\n",
        "\n",
        "def rf_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler):\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = \"\"\n",
        "    csv_directory = \"\"\n",
        "\n",
        "    model = load(model_directory + \"\" + set_name + \"_rf_\" + str(future) + \".pkl\")\n",
        "    predictions = model.predict(X_test).reshape(-1, 1)\n",
        "    predictions = y_scaler.inverse_transform(predictions)\n",
        "    y_test = y_scaler.inverse_transform(y_test)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"rf\")\n",
        "\n",
        "    print(f\"Finished running rf prediction on future window {0}\", future)\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"rf\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def rf_evaluate(future, set_name, X_train, y_train, epochs, epd):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = \"\"\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "    rf_train_model(future, epochs,\n",
        "            model_directory, set_name, X_train, y_train, epd)\n",
        "    \n",
        "    print(\"Finished evaluating rf for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    return time_end - time_start\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "aTT9eE-STMrg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_train_model(future, epochs, model_directory, set_name, X_train, y_train, epd):\n",
        "    \n",
        "    split = 0.9\n",
        "\n",
        "    length = X_train.shape[0]\n",
        "    X_train_temp = X_train[:int(length * split), :]\n",
        "    y_train_temp = y_train[:int(length * split), :]\n",
        "    X_val = X_train[int(length * split):, :]\n",
        "    y_val = y_train[int(length * split):, :]\n",
        "\n",
        "    tss = TimeSeriesSplit(n_splits=5, test_size=epd*90, gap=0)\n",
        "    estimator = xgb.XGBRegressor(booster='gbtree',    \n",
        "            early_stopping_rounds=50,\n",
        "            objective='reg:squarederror',\n",
        "            verbosity=0)\n",
        "\n",
        "    search_space = {\n",
        "        \"learning_rate\": (0.01, 1.0, \"log-uniform\"),\n",
        "        \"min_child_weight\": (0, 10),\n",
        "        \"max_depth\": (1, 50),\n",
        "        \"subsample\": (0.01, 1.0, \"uniform\"),\n",
        "        \"colsample_bytree\": (0.01, 1.0, \"log-uniform\"),\n",
        "        \"reg_lambda\": (1e-9, 1.0, \"log-uniform\"),\n",
        "        \"reg_alpha\": (1e-9, 1.0, \"log-uniform\"),\n",
        "        \"gamma\": (1e-9, 0.5, \"log-uniform\"),\n",
        "        \"n_estimators\": (5, 5000),\n",
        "    }\n",
        "\n",
        "    model = BayesSearchCV(\n",
        "        estimator=estimator,\n",
        "        search_spaces=search_space,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        cv=tss,\n",
        "        n_jobs=-1,\n",
        "        n_iter=epochs,\n",
        "        verbose=0,\n",
        "        refit=True,\n",
        "    )\n",
        "\n",
        "    model = model.fit(X_train_temp, y_train_temp, eval_set=[(X_val, y_val)], verbose=False)\n",
        "    dump(model, model_directory + \"\" + set_name + \"_xgb_\" + str(future) + \".pkl\")\n",
        "\n",
        "\n",
        "def xgb_predict(future, set_name, pred_dates_test, X_test, y_test, y_scaler):\n",
        "\n",
        "    folder_path = os.getcwd()\n",
        "    model_directory = \"\"\n",
        "    csv_directory = \"\"\n",
        "\n",
        "    model = load(model_directory + \"\" + set_name + \"_xgb_\" + str(future) + \".pkl\")\n",
        "    predictions = model.predict(X_test).reshape(-1, 1)\n",
        "    predictions = y_scaler.inverse_transform(predictions)\n",
        "    y_test = y_scaler.inverse_transform(y_test)\n",
        "\n",
        "    make_csvs(csv_directory, predictions, y_test, pred_dates_test, set_name, future, \"xgb\")\n",
        "\n",
        "    print(\"Finished running xgb prediction on future window {0}\".format(future))\n",
        "\n",
        "    metric_outputs = get_metrics(predictions, y_test, 0, \"xgb\")\n",
        "    return metric_outputs\n",
        "\n",
        "\n",
        "def xgb_evaluate(future, set_name, X_train, y_train, epochs, epd):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    model_directory = \"\"\n",
        "\n",
        "    absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "    xgb_train_model(future, epochs,\n",
        "            model_directory, set_name, X_train, y_train, epd)\n",
        "    \n",
        "    print(\"Finished evaluating xgb for future {0}\".format(future))\n",
        "\n",
        "    time_end = time.time()\n",
        "    return time_end - time_start\n",
        "    \n"
      ],
      "metadata": {
        "id": "j92gLycCTPrI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__==\"__main__\":\n",
        "\n",
        "try:\n",
        "    \n",
        "    csv_directory = \"\"\n",
        "    print(csv_directory)\n",
        "    \n",
        "    data = pd.read_excel(csv_directory + r'ausdata.xlsx').set_index(\"Date\")\n",
        "    holidays = pd.read_excel(csv_directory + r'Holidays2.xls')\n",
        "\n",
        "    data['Holiday'] = data.index.isin(holidays['Date']).astype(int)\n",
        "\n",
        "    file_name = csv_directory + \"matlab_temp.xlsx\"\n",
        "    data.to_excel(file_name)\n",
        "\n",
        "except FileNotFoundError:\n",
        "\n",
        "    print(\"Ausdata and Holidays2 xl files are not present in \\\"csvs\\\" directory.\")\n",
        "    print(\"Ensure they are before continuing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEcDd36WUhWK",
        "outputId": "ed796aa1-dc88-48ed-8d04-a0d85e60a73b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__==\"__main__\":\n",
        "\n",
        "csv_directory = \"\"\n",
        "\n",
        "# These are the values that need changing per different dataset\n",
        "file_path = \"matlab_temp.xlsx\"\n",
        "set_name = \"matlab\"\n",
        "target = \"SYSLoad\"\n",
        "trend_type = \"Additive\"\n",
        "epd = 48\n",
        "future = 7\n",
        "\n",
        "cleaning = 1\n",
        "training = 1\n",
        "predicting = 1\n",
        "eval_tpot = 0\n",
        "\n",
        "partition = 5000\n",
        "data_epochs = 10\n",
        "\n",
        "# Don't know what else to put in cleaning parameters tbh\n",
        "# put more research into this area\n",
        "cleaning_parameters = {\n",
        "  # This is seriously jank. It works for now, but golly...\n",
        "  'pca_dimensions': [None, math.inf, -math.inf],\n",
        "  'scalers': ['standard', 'minmax']\n",
        "}\n",
        "\n",
        "window = 10\n",
        "split = 0.8\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "if cleaning:\n",
        "\n",
        "  data, outputs = feature_adder(csv_directory, file_path, target, trend_type, future, epd,  set_name)\n",
        "\n",
        "  # Decide on exactly what size this partition should be\n",
        "  # Essentially this grid search is shit and has to be optimized later on down the track\n",
        "  # It'll just get the job done for now\n",
        "  best_results = data_cleaning_pipeline(data[:partition], outputs[:partition], cleaning_parameters, target, split, data_epochs, batch_size, csv_directory)\n",
        "\n",
        "else:\n",
        "\n",
        "  if os.path.exists(csv_directory + \"best_data_parameters.csv\") \\\n",
        "          and os.path.exists(csv_directory + \"\" + set_name + \"_data_\" + str(future) + \".csv\") \\\n",
        "          and os.path.exists(csv_directory + \"\" + set_name + \"_outputs_\" + str(future) + \".csv\"):\n",
        "      \n",
        "      best_results = pd.read_csv(csv_directory + \"best_data_parameters.csv\").to_dict('index')\n",
        "      best_results = best_results.get(0)\n",
        "      data, outputs = load_datasets(csv_directory, set_name, future)\n",
        "\n",
        "  else:\n",
        "      data, outputs = feature_adder(csv_directory, file_path, target, trend_type, future, epd,  set_name)\n",
        "\n",
        "      # Decide on exactly what size this partition should be\n",
        "      # Essentially this grid search is shit and has to be optimized later on down the track\n",
        "      # It'll just get the job done for now\n",
        "      best_results = data_cleaning_pipeline(data[:partition], outputs[:partition], cleaning_parameters, target, split, data_epochs, batch_size, csv_directory)\n",
        "\n",
        "print(\"finished cleaning\")\n",
        "X_frame, y_data, pred_dates, y_scaler = finalise_data(data, outputs, target, best_results)\n",
        "length = X_frame.shape[0]\n",
        "\n",
        "pred_dates_test = pred_dates[int(length*split) + window:]\n",
        "\n",
        "X_2d = create_dataset_2d(X_frame, window)\n",
        "X_3d = create_dataset_3d(X_frame, window)\n",
        "\n",
        "y_test = y_data[int(length*split) + window:]\n",
        "X_test_2d = X_2d[int(length*split):]\n",
        "X_test_3d = X_3d[int(length*split):]\n",
        "\n",
        "y_train = y_data[window:int(length*split) + window]\n",
        "X_train_2d = X_2d[:int(length * split)]\n",
        "X_train_3d = X_3d[:int(length * split)]\n",
        "\n",
        "if training:\n",
        "\n",
        "  np.save(\"X_train_3d.npy\", X_train_3d)\n",
        "\n",
        "  bnn_time = bnn_evaluate(future, set_name, X_train_2d, y_train, epochs, batch_size, y_scaler, epd)\n",
        "  cnn_time = cnn_evaluate(future, set_name, X_train_3d, y_train, epochs, batch_size, y_scaler, epd)\n",
        "  xgb_time = xgb_evaluate(future, set_name, X_train_2d, y_train, epochs, epd)\n",
        "  rf_time = rf_evaluate(future, set_name, X_train_2d, y_train.reshape(-1), epochs, epd)\n",
        "  base_time = simple_evaluate(future, set_name, X_train_2d, y_train, epochs, batch_size)\n",
        "\n",
        "if predicting:\n",
        "\n",
        "  bnn_metrics = bnn_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler)\n",
        "  cnn_metrics = cnn_predict(future, set_name, pred_dates_test, X_test_3d, y_test, y_scaler)\n",
        "  xgb_metrics = xgb_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler)\n",
        "  rf_metrics = rf_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler)\n",
        "  base_metrics = simple_predict(future, set_name, pred_dates_test, X_test_2d, y_test, y_scaler)\n",
        "\n",
        "  if training:\n",
        "      bnn_metrics['TIME'] = bnn_time\n",
        "      cnn_metrics['TIME'] = cnn_time\n",
        "      xgb_metrics['TIME'] = xgb_time\n",
        "      rf_metrics['TIME'] = rf_time\n",
        "      base_metrics['TIME'] = base_time\n",
        "\n",
        "  metrics = [bnn_metrics, cnn_metrics, xgb_metrics, rf_metrics, base_metrics]\n",
        "  metrics = normalise_metrics(metrics, training)\n",
        "\n",
        "  metrics = {\"Basic_nn\": metrics[0], \"Complex_nn\": metrics[1], \"xgb\": metrics[2], \n",
        "              \"rf\": metrics[3], \"Baseline\": metrics[4]}\n",
        "\n",
        "  make_metrics_csvs(csv_directory, metrics, set_name, future, training)\n",
        "\n",
        "# if eval_tpot:\n",
        "#     # Problem still exists with tpot for whatever reason\n",
        "#     tpot_evaluate(future, set_name, X_train_2d, y_train.reshape(-1), pred_dates_test, X_test_2d, y_test.reshape(-1), y_scaler)\n",
        "\n",
        "if os.path.exists(\"X_train_3d.npy\"):\n",
        "  os.remove(\"X_train_3d.npy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "gzWpa_klTdtx",
        "outputId": "e0c93e6d-2458-42d6-c7df-e1b1fc2db21d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved future window 7 to csvs\n",
            "Trained scale:standard dim:None\n",
            "Trained scale:standard dim:inf\n",
            "Trained scale:standard dim:-inf\n",
            "Trained scale:minmax dim:None\n",
            "Trained scale:minmax dim:inf\n",
            "Trained scale:minmax dim:-inf\n",
            "finished cleaning\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2bafb5b0b346>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train_3d.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0mbnn_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnn_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m   \u001b[0mcnn_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0mxgb_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2051eea2c5bf>\u001b[0m in \u001b[0;36mbnn_evaluate\u001b[0;34m(future, set_name, X_train, y_train, epochs, batch_size, y_scaler, epd)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     bnn_train_model(future, batch_size, epochs,\n\u001b[0m\u001b[1;32m    130\u001b[0m             model_directory, set_name, X_train, y_train, y_scaler, epd)\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2051eea2c5bf>\u001b[0m in \u001b[0;36mbnn_train_model\u001b[0;34m(future, batch_size, epochs, model_directory, set_name, X_train, y_train, y_scaler, epd)\u001b[0m\n\u001b[1;32m     60\u001b[0m                     restore_best_weights=True)\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     tuner.search(X_train, y_train, verbose=0, epochs=epochs, validation_split=0.2, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     63\u001b[0m                 callbacks=[monitor])\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_hypermodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         tuner_utils.validate_trial_results(\n\u001b[1;32m    216\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperModel.fit()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ML-IYc4TbAnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}